{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine Learning Live](images/title-slide.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalab.bigquery as bq\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict an output based on known inputs\n",
    "\n",
    "* rate/demand forecasts\n",
    "* probability of things like fraud or conversions\n",
    "* package delivery times\n",
    "* medical, eg: mortality rates for risk groups\n",
    "* insurance risk\n",
    "* operations/monitoring\n",
    "\n",
    "## traditional rules\n",
    "\n",
    "~~~~\n",
    "                   rules\n",
    "                     ||\n",
    "                     \\/\n",
    "             ----------------\n",
    " input ====> |  algorithm   |  =====> output\n",
    "             ----------------\n",
    "~~~~\n",
    "\n",
    "## With Machine Learning\n",
    "\n",
    "~~~~\n",
    "                    data\n",
    "                     ||\n",
    "                     \\/\n",
    "             ----------------\n",
    " input ====> |   ML model   |  =====> output\n",
    "             ----------------\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Taxi Fares?\n",
    "\n",
    "The city of [New York published a dataset](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml) with taxi > 150M rides per year since 2009. This makes it a great dataset for training because:\n",
    "\n",
    "* one can easily relate to the data (we have all used taxis before)\n",
    "* simple enough, one table with a handful of columns\n",
    "* large enough to do relevant machine learning\n",
    "* there are lots of blogs, notebooks etc from others about all aspects of this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi fares\n",
    "\n",
    "Sounds easy enough to do with rules:\n",
    "\n",
    "* initial charge \\$2.50\n",
    "* \\$0.40 per 1/5 mile\n",
    "* \\$0.40 per 1 minute stopped/slow traffic\n",
    "* \\$1.00 Weekday Surcharge 4pm-8pm\n",
    "* \\$0.50 Night Surcharge 8pm-6am\n",
    "\n",
    "But to calculate this before a trip you need to already know the route time and distance. We will use this route from LGA to midtown Manhattan as an example and you can see that there are different route options with different estimated times and mileages. These also vary highly with the amount of traffic over the course of a day.\n",
    "\n",
    "Problem - fare increases.\n",
    "\n",
    "![sample route LGA to midtown](images/sample_route.png \"Sample Route\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick look at the data\n",
    "\n",
    "The dataset that we will use is a public data and conveniently available at several locations, including a <a href=\"https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips\">BigQuery public dataset</a>. \n",
    "\n",
    "Let's write a SQL query to poke around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql --module afewrecords2\n",
    "SELECT\n",
    "  pickup_datetime,\n",
    "  pickup_longitude, pickup_latitude, \n",
    "  dropoff_longitude, dropoff_latitude,\n",
    "  passenger_count,\n",
    "  trip_distance,\n",
    "  tolls_amount,\n",
    "  fare_amount,\n",
    "  total_amount\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  ABS(HASH(pickup_datetime)) % $EVERY_N == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = bq.Query(afewrecords2, EVERY_N=100000).to_dataframe()\n",
    "trips[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do with this dataset?\n",
    "\n",
    "* reporting\n",
    "* analytics and exploration\n",
    "* predictive analytics\n",
    "\n",
    "But first we should do some data wrangling to improve the quality of the dataset we work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the number of records so that we can do some neat graphs. There is no guarantee about the order in which records are returned, and so no guarantee about which records get returned if we simply increase the LIMIT. To properly sample the dataset, let's use the HASH of the pickup time and return 1 in 100,000 records -- because there are 1 billion records in the data, we should get back approximately 10,000 records if we do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exploring data </h3>\n",
    "\n",
    "Let's explore this dataset and clean it up as necessary. We'll use the Python Seaborn package to visualize graphs and Pandas to do the slicing and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "ax = sns.regplot(x=\"trip_distance\", y=\"fare_amount\", fit_reg=False, ci=None, truncate=True, data=trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm ... do you see something wrong with the data that needs addressing?\n",
    "\n",
    "It appears that we have a lot of invalid data that is being coded as zero distance and some fare amounts that are definitely illegitimate. Let's remove them from our analysis. We can do this by modifying the BigQuery query to keep only trips longer than zero miles and fare amounts that are at least the minimum cab fare ($2.50).\n",
    "\n",
    "Note the extra WHERE clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql --module afewrecords3\n",
    "SELECT\n",
    "  pickup_datetime,\n",
    "  pickup_longitude, pickup_latitude, \n",
    "  dropoff_longitude, dropoff_latitude,\n",
    "  passenger_count,\n",
    "  trip_distance,\n",
    "  tolls_amount,\n",
    "  fare_amount,\n",
    "  total_amount\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  (ABS(HASH(pickup_datetime)) % $EVERY_N == 1 AND\n",
    "  trip_distance > 0 AND fare_amount >= 2.5 AND fare_amount <120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = bq.Query(afewrecords3, EVERY_N=100000).to_dataframe()\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15, 10)\n",
    "ax = sns.regplot(x=\"trip_distance\", y=\"fare_amount\", fit_reg=False, ci=None, truncate=True, data=trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's up with the streaks at <span>$</span>45 and \\$50?  Those are fixed-amount rides from JFK into anywhere in Manhattan, i.e. to be expected. Let's list the data to make sure the values look reasonable.\n",
    "\n",
    "Let's examine whether the toll amount is captured in the total amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tollrides = trips[trips['tolls_amount'] > 0]\n",
    "tollrides[tollrides['pickup_datetime'] == '2012-09-05 15:45:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking a few samples above, it should be clear that the total amount reflects fare amount, toll and tip somewhat arbitrarily -- this is because when customers pay cash, the tip is not known.  So, we'll use the sum of fare_amount + tolls_amount as what needs to be predicted.  Tips are discretionary and do not have to be included in our fare estimation tool.\n",
    "\n",
    "Let's also look at the distribution of values within the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Quality control and other preprocessing </h3>\n",
    "\n",
    "We need to some clean-up of the data:\n",
    "<ol>\n",
    "<li>New York city longitudes are around -74 and latitudes are around 41.</li>\n",
    "<li>We shouldn't have zero passengers.</li>\n",
    "<li>Clean up the total_amount column to reflect only fare_amount and tolls_amount, and then remove those two columns.</li>\n",
    "<li>Before the ride starts, we'll know the pickup and dropoff locations, but not the trip distance (that depends on the route taken), so remove it from the ML dataset</li>\n",
    "<li>Discard the timestamp</li>\n",
    "</ol>\n",
    "\n",
    "We could do preprocessing in SQL, similar to how we removed the zero-distance rides, but just to show you another option, let's do this in Python.  In production, we'll have to carry out the same preprocessing on the real-time input data. \n",
    "\n",
    "This sort of preprocessing of input data is quite common in ML, especially if the quality-control is dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(trips_in):\n",
    "  trips = trips_in.copy(deep=True)\n",
    "  trips.fare_amount = trips.fare_amount + trips.tolls_amount\n",
    "  del trips['tolls_amount']\n",
    "  del trips['total_amount']\n",
    "  del trips['trip_distance']\n",
    "  del trips['pickup_datetime']\n",
    "  qc = np.all([\\\n",
    "             trips['pickup_longitude'] > -78, \\\n",
    "             trips['pickup_longitude'] < -70, \\\n",
    "             trips['dropoff_longitude'] > -78, \\\n",
    "             trips['dropoff_longitude'] < -70, \\\n",
    "             trips['pickup_latitude'] > 37, \\\n",
    "             trips['pickup_latitude'] < 45, \\\n",
    "             trips['dropoff_latitude'] > 37, \\\n",
    "             trips['dropoff_latitude'] < 45, \\\n",
    "             trips['passenger_count'] > 0,\n",
    "            ], axis=0)\n",
    "  return trips[qc]\n",
    "\n",
    "trips = bq.Query(afewrecords3, EVERY_N=10000).to_dataframe()\n",
    "\n",
    "tripsqc = preprocess(trips)\n",
    "tripsqc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scoring of accuracy\n",
    "\n",
    "To compare the quality of different models we have to come up with a way to express the overall accuracy of a model. To do that we will run predictions against the same test training set and compare our predictions agains the known correct values. A simple way to aggregate all individual deviations or errors is the Root Mean Squared Error (RMSE). The calculation is\n",
    "\n",
    "![rmse_formula](https://wikimedia.org/api/rest_v1/media/math/render/svg/eeb88fa0f90448e9d1a67cd7a70164f674aeb300 \"RMSE formula\")\n",
    "\n",
    "The benefit of RMSE is that it is easy to calculate and the result is expressed in the same unit as the predicted label. Throughout the training phase(s) the model will \"fit\" against the training set. We can calculate the RMSE of a model against the training set but it will usually be biased to be too optimistic as the model might be suffering from overfitting, that is it might be good at \"remembering\" certain outcomes in the test data or handle it's specific quirks but then underperform against real life data.\n",
    "That's why it is necessary to evaluate the performance of a model against a validation set. More sophisticated ML models and neural networks have many parameters that can be tuned either by an engineer or the training process itself. In those cases, each iteration of a model training would check the results against the validation set, adjust a parameter, then retrain and see if the new parameter value improved or regressed the model's performance.\n",
    "\n",
    "There are many options for choosing loss functions and they can have a significant impact on the performance of trained ML models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create ML datasets </h3>\n",
    "\n",
    "Let's split the QCed data randomly into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = tripsqc.sample(frac=1)\n",
    "trainsize = int(len(shuffled['fare_amount']) * 0.80)\n",
    "validsize = int(len(shuffled['fare_amount']) * 0.20)\n",
    "\n",
    "\n",
    "df_train = shuffled.iloc[:trainsize, :]\n",
    "df_valid = shuffled.iloc[trainsize:(trainsize+validsize), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out the two dataframes to appropriately named csv files. We can use these csv files for local training (recall that these files represent only 1/100,000 of the full dataset) until we get to point of using Dataflow and Cloud ML.\n",
    "\n",
    "The training set will be used to fit the model in the training phase.\n",
    "Validation is for quick evaluation of the trained model, especially useful in comparing different training parameters.\n",
    "It would also be best practice to have a Test dataset as the final step and used to compare different models against each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(df, filename):\n",
    "  outdf = df.copy(deep=False)\n",
    "  outdf.loc[:, 'key'] = np.arange(0, len(outdf)) # rownumber as key\n",
    "  # reorder columns so that target is first column\n",
    "  cols = outdf.columns.tolist()\n",
    "  cols.remove('fare_amount')\n",
    "  cols.insert(0, 'fare_amount')\n",
    "  print (cols)  # new order of columns\n",
    "  outdf = outdf[cols]\n",
    "  outdf.to_csv(filename, header=False, index_label=False, index=False)\n",
    "\n",
    "! mkdir -p data\n",
    "to_csv(df_train, 'data/taxi-train.csv')\n",
    "to_csv(df_valid, 'data/taxi-valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 data/taxi-train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 .csv files corresponding to training and validation.  The ratio of file-sizes correspond to our split of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! We now have our ML datasets and are ready to train ML models, validate them and evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rule-based Benchmark\n",
    "\n",
    "Before we start building actual ML models, it is a good idea to come up with a very simple model and use that as a benchmark.\n",
    "\n",
    "The simplest model is going to be to derive the trip distance and calculate the mean rate per km (or mile). Then to predict the rate we multiply that average cost per km with the trip distance.\n",
    "In the absence of a map routing engine, we will have to use the direct line between pickup and dropoff which is obviously flawed but simple and fast to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_between(lat1, lon1, lat2, lon2):\n",
    "  # haversine formula to compute distance \"as the crow flies\".  Taxis can't fly of course.\n",
    "  dist = np.degrees(np.arccos(np.minimum(1,np.sin(np.radians(lat1)) * np.sin(np.radians(lat2)) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.cos(np.radians(lon2 - lon1))))) * 60 * 1.515 * 1.609344\n",
    "  return dist\n",
    "\n",
    "def estimate_distance(df):\n",
    "  return distance_between(df['pickuplat'], df['pickuplon'], df['dropofflat'], df['dropofflon'])\n",
    "\n",
    "def compute_rmse(actual, predicted):\n",
    "  return np.sqrt(np.mean((actual-predicted)**2))\n",
    "\n",
    "def print_rmse(df, rate, name):\n",
    "  print (\"{1} RMSE = {0}\".format(compute_rmse(df['fare_amount'], rate*estimate_distance(df)), name))\n",
    "\n",
    "FEATURES = ['pickuplon','pickuplat','dropofflon','dropofflat','passengers']\n",
    "TARGET = 'fare_amount'\n",
    "columns = list([TARGET])\n",
    "columns.extend(FEATURES) # in CSV, target is the first column, after the features\n",
    "columns.append('key')\n",
    "df_train = pd.read_csv('data/taxi-train.csv', header=None, names=columns)\n",
    "df_valid = pd.read_csv('data/taxi-valid.csv', header=None, names=columns)\n",
    "rate = df_train['fare_amount'].mean() / estimate_distance(df_train).mean()\n",
    "print (\"Rate = ${0}/km\".format(rate))\n",
    "print_rmse(df_train, rate, 'Train')\n",
    "#print_rmse(df_valid, rate, 'Valid')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple distance-based rule gives us a RMSE of about <b>$8</b>.  We have to beat this, of course, but you will find that simple rules of thumb like this can be surprisingly difficult to beat.\n",
    "\n",
    "Let's be ambitious, though, and make our goal to build ML models that have a RMSE of less than $6 on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning using Tensorflow\n",
    "\n",
    "Tensorflow is a popular open source machine learning library. Beyond basic algorithms it supports distributed training on neural networks. We will start with a linear regression model using tf.estimator and evaluate its performance. Tensorflow can run both locally or on a remote compute cluster. We will start with a small dataset (<10k records) so we can do it all in-memory on the VM that this notebook is running on. We will also just pass the data in as-is. \n",
    "\n",
    "## A quick word about linear regression\n",
    "\n",
    "The model is a simple linear equation:\n",
    "\n",
    "y = c + a1*x1 + a2*x2 + ...\n",
    "\n",
    "And during fitting/trainig we are trying out different weights to minimize overall loss. In the simplest form all inputs to the equation must be numerical for this to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In CSV, label is the first column, after the features, followed by the key\n",
    "CSV_COLUMNS = ['fare_amount', 'pickuplon','pickuplat','dropofflon','dropofflat','passengers', 'key']\n",
    "FEATURES = CSV_COLUMNS[1:len(CSV_COLUMNS) - 1]\n",
    "LABEL = CSV_COLUMNS[0]\n",
    "\n",
    "df_train = pd.read_csv('./data/taxi-train.csv', header = None, names = CSV_COLUMNS)\n",
    "df_valid = pd.read_csv('./data/taxi-valid.csv', header = None, names = CSV_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input function to read from Pandas Dataframe into tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_fn(df, num_epochs):\n",
    "  return tf.estimator.inputs.pandas_input_fn(\n",
    "    x = df,\n",
    "    y = df[LABEL],\n",
    "    batch_size = 128,\n",
    "    num_epochs = num_epochs,\n",
    "    shuffle = True,\n",
    "    queue_capacity = 1000,\n",
    "    num_threads = 1\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature columns for estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_cols():\n",
    "  input_columns = [tf.feature_column.numeric_column(k) for k in FEATURES]\n",
    "  return input_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Linear Regression with tf.Estimator framework </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "OUTDIR = 'taxi_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "model = tf.estimator.LinearRegressor(\n",
    "      feature_columns = make_feature_cols(), model_dir = OUTDIR)\n",
    "\n",
    "model.train(input_fn = make_input_fn(df_train, num_epochs = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the validation data. Let's see if the number of iterations can help to improve the training. Try out different values and calculate the RMSE for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rmse(model, name, df):\n",
    "  metrics = model.evaluate(input_fn = make_input_fn(df, 1))\n",
    "  print('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\n",
    "    \n",
    "print_rmse(model, 'validation', df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nowhere near our benchmark (RMSE of $8 or so on this data), but it serves to demonstrate what TensorFlow code looks like.  Let's use this model for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network regression\n",
    "\n",
    "http://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "model = tf.estimator.DNNRegressor(hidden_units = [32, 8, 2],\n",
    "      feature_columns = make_feature_cols(), model_dir = OUTDIR)\n",
    "model.train(input_fn = make_input_fn(df_train, num_epochs = 20));\n",
    "\n",
    "print_rmse(model, 'valid', df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not beating our benchmark with either model ... what's up?  Well, we may be using TensorFlow for Machine Learning, but we are not yet using it well.  That's what the rest of this session is about!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE on benchmark dataset is > $10 (results will vary because of random seeds).\n",
    "\n",
    "This is not only way more than our goal of 6.00, but it doesn't even beat our distance-based rule's RMSE of 8.\n",
    "\n",
    "Fear not -- you have learned how to write a TensorFlow model, but not to do all the things that you will have to do to your ML model performant. We will do this in the next chapters. In this chapter though, we will get our TensorFlow model ready for these improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving ML\n",
    "\n",
    "## better data: Feature engineering\n",
    "\n",
    "### Day of week\n",
    "\n",
    "* date is available for training and inference\n",
    "* exact date (YYYY-MM-DD) is not that useful as we would have different values between training and inference\n",
    "* but we can extract the day of the week and there is likely a correlation between traffic on different dates\n",
    "* It's also important to note that this should not be a numerical feature. day=4 is not any better than day=2\n",
    "* We can code the dow as a categorical feature which will turn one input into 7 distinct features\n",
    "\n",
    "### Hour of Day\n",
    "\n",
    "* the pickup time likely also has an impact on traffic and fares (we know that there are time-based surcharges)\n",
    "* but we also know that there is likely not much difference between a pickup at 12:30 vs 12:42\n",
    "* so let's settle on coding the hour of the day as another category\n",
    "* furthermore we can create a feature-cross between hour-of-day and day-of-week\n",
    "\n",
    "### distance\n",
    "\n",
    "* we saw from our initial benchmark that the euclidian distance is a feature with a strong correlation\n",
    "* that distance can easily be calculated from the input long/lat values\n",
    "\n",
    "### pickup and dropoff\n",
    "\n",
    "* our dataset has very (too?) accurate location data\n",
    "* but we know that if pickup and dropoff are within a few city blocks, the fare should be about the same\n",
    "* an extra challenge we do not address: bridges and tolls\n",
    "* what we can easily do is to create a grid of buckets over the city's coordinates\n",
    "* we can make that a tunable parameter, starting with a 10x10 grid\n",
    "* additionally let's add a feature-cross of pickup and dropoff so that we can group similar trips together\n",
    "\n",
    "## hyperparameter tuning\n",
    "\n",
    "* we already saw that the layout of neurons plays a great role in finding the best DNN\n",
    "* likewise a different number of buckets for the location grid can have an impact\n",
    "* we are going to search for the optimal values\n",
    "\n",
    "## more data: scale to cloud training\n",
    "\n",
    "* DNNs generally create better predictions the larger the training dataset becomes\n",
    "* we will perform a few dry-runs locally\n",
    "* and then increase the amount of training data and fit on a cluster of machines in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'rostlab-181304'    # CHANGE THIS\n",
    "BUCKET = 'rostlab-181304-ml' # REPLACE WITH YOUR BUCKET NAME. Use a regional bucket in the region you selected.\n",
    "REGION = 'us-central1' # Choose an available region for Cloud MLE from https://cloud.google.com/ml-engine/docs/regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## ensure gcloud is up to date\n",
    "gcloud components update\n",
    "\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "\n",
    "## ensure we predict locally with our current Python environment\n",
    "gcloud config set ml_engine/local_python `which python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop model with new inputs\n",
    "\n",
    "Download the first shard of a preprocessed data to enable local development. We could create these files ourselves but they are also available on a public bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d sample ]; then\n",
    "  rm -rf sample\n",
    "fi\n",
    "mkdir sample\n",
    "\n",
    "gsutil cp \"gs://$BUCKET/taxifare/taxi_preproc/train.csv-00000-of*\" sample/train.csv\n",
    "gsutil cp \"gs://$BUCKET/taxifare/taxi_preproc/valid.csv-00000-of*\" sample/valid.csv\n",
    "wc -l sample/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two new inputs in the INPUT_COLUMNS, three engineered features, and the estimator involves bucketization and feature crosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep -A 20 \"INPUT_COLUMNS =\" taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep -A 50 \"build_estimator\" taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep -A 15 \"add_engineered(\" taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "python -m trainer.task \\\n",
    "  --train_data_paths=${PWD}/sample/train.csv \\\n",
    "  --eval_data_paths=${PWD}/sample/valid.csv  \\\n",
    "  --output_dir=${PWD}/taxi_trained \\\n",
    "  --train_steps=10 \\\n",
    "  --job-dir=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE = {}'.format(np.sqrt(74)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train on cloud\n",
    "\n",
    "This will take <b> 5-10 minutes </b> even though the prompt immediately returns after the job is submitted. Monitor job progress on the [Cloud Console, in the ML Engine](https://console.cloud.google.com/mlengine) section and wait for the training job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/taxifare/taxi_trained\n",
    "JOBNAME=ml_live_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=${PWD}/taxifare/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=STANDARD_1 \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --train_data_paths=\"gs://${BUCKET}/taxifare/taxi_preproc/train.csv-00000-of-*\" \\\n",
    "  --eval_data_paths=\"gs://${BUCKET}/taxifare/taxi_preproc/valid.csv-00000-of-*\"  \\\n",
    "  --train_steps=5000 \\\n",
    "  --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is now 8.33249, an improvement over the 9.3 that we were getting ... of course, we won't know until we train/validate on a larger dataset. Still, this is promising. But before we do that, let's do hyper-parameter tuning.\n",
    "\n",
    "<b>Use the Cloud Console link to monitor the job and do NOT proceed until the job is done.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter tune\n",
    "\n",
    "Training a DNN supports a number of parameters such as the number of hidden layers, training batch size etc. These have a significant impact on a model's performance. Hypertuning is the process of iteratively trying out several options to come up with the optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command-line parameters to task.py\n",
    "\n",
    "Note the command-line parameters to task.py.  These are the things that could be hypertuned if we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -A 2 add_argument taxifare/trainer/task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric\n",
    "\n",
    "We add a special evaluation metric. It could be any objective function we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -A 5 get_eval_metrics taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add trial id to not overwrite existing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -A 5 \"trial\" taxifare/trainer/task.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameter configuration\n",
    "\n",
    "The file specifies the search region in parameter space.  Cloud MLE carries out a smart search algorithm within these constraints (i.e. it does not try out every single value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: STANDARD_1\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 30\n",
    "    maxParallelTrials: 3\n",
    "    hyperparameterMetricTag: rmse\n",
    "    params:\n",
    "    - parameterName: train_batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 64\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: nbuckets\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 20\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: hidden_units\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\"128 32\", \"256 128 16\", \"64 64 64 8\"]       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the HP training job\n",
    "we just add the new HP config to the existing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/taxifare/taxi_hypertrain\n",
    "JOBNAME=ml_live_hp_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/taxifare/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   --runtime-version=$TFVERSION \\\n",
    "   --config=hyperparam.yaml \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/taxifare/taxi_preproc/train.csv-00000-of-*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/taxifare/taxi_preproc/valid.csv-00000-of-*\"  \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --train_steps=2500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this we come up with these values for our training parameters:\n",
    "\n",
    "* train_batch_size: 512\n",
    "* nbuckets: 16\n",
    "* hidden_units: \"64 64 64 8\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/taxifare/feateng\n",
    "JOBNAME=mllive_$(date -u +%y%m%d_%H%M%S)\n",
    "TIER=STANDARD_1 \n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "#gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/taxifare/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=$TIER \\\n",
    "   --runtime-version=$TFVERSION \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/taxifare/taxi_preproc/train.csv-00001-of-*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/taxifare/taxi_preproc/valid.csv-00000-of-*\"  \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --train_steps=8000 \\\n",
    "   --train_batch_size=512 --nbuckets=16 --hidden_units=\"64 64 64 8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields an RMSE of about \\$5. Beating our first benchmark. But let's try this on a larger dataset and see how far we can push it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on large dataset with 2M records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WARNING -- this uses significant resources and is optional. Remove this line to run the block.\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/taxifare/feateng2m\n",
    "JOBNAME=mllivexl_$(date -u +%y%m%d_%H%M%S)\n",
    "TIER=STANDARD_1 \n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "# only remove the outdir if you don't want to resume a previous run\n",
    "#gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/taxifare/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=$TIER \\\n",
    "   --runtime-version=$TFVERSION \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/taxifare/taxi_preproc/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/taxifare/taxi_preproc/valid*\"  \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --train_steps=1600000 \\\n",
    "   --train_batch_size=512 --nbuckets=16 --hidden_units=\"64 64 64 8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "OUTDIR='gs://{0}/taxifare/feateng2m'.format(BUCKET)\n",
    "print(OUTDIR)\n",
    "TensorBoard().start(OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_df = TensorBoard.list()\n",
    "if not pids_df.empty:\n",
    "    for pid in pids_df['pid']:\n",
    "        TensorBoard().stop(pid)\n",
    "        print('Stopped TensorBoard with pid {}'.format(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE after training on the 2-million-row dataset is below \\$4.  This graph shows the improvements so far ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Lab' : pd.Series(['1a', '2-3', '4a', '4b', '4c']),\n",
    "              'Method' : pd.Series(['Heuristic Benchmark', 'tf.learn', '+Feature Eng.', '+ Hyperparam', '+ 2m rows']),\n",
    "              'RMSE': pd.Series([8.026, 9.4, 8.3, 5.0, 3.03]) })\n",
    "\n",
    "ax = sns.barplot(data = df, x = 'Method', y = 'RMSE')\n",
    "ax.set_ylabel('RMSE (dollars)')\n",
    "ax.set_xlabel('Labs/Methods')\n",
    "plt.plot(np.linspace(-20, 120, 1000), [5] * 1000, 'b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model for prediction\n",
    "\n",
    "Finally, let's use the trained model to predict the taxi fare to get me from my hotel to the Javits Center for Oracle Code NYC. First, we write the trip data to a file, then we run the prediction locally using the exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /tmp/test.json\n",
    "{\"dayofweek\": \"Tue\", \"hourofday\": 10, \"pickuplon\": -73.976055, \"pickuplat\": 40.763492, \"dropofflon\": -74.001891, \"dropofflat\": 40.757328, \"passengers\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "model_dir=$(gsutil ls gs://${BUCKET}/taxifare/feateng2m/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "  --model-dir=${model_dir} \\\n",
    "  --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "This notebook is based on labs from an excellent [coursera class](https://www.coursera.org/learn/serverless-machine-learning-gcp) that is a great starting point to learn about machine learning with tensorflow in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
