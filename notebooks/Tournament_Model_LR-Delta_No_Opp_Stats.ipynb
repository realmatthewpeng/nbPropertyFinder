{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import operator\n",
    "# Non pythonic hack to reuse some utility code\n",
    "if sys.path[0] != '../py_utils':\n",
    "    sys.path.insert(0,'../py_utils')\n",
    "\n",
    "import file_utils    \n",
    "import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)\n",
    "print(\"Seaborn version: \", sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missed_predictions(tourney_comp_ratings, model_features, numeric_feature_to_scale,\n",
    "                           prediction_probabilities, X, y, y_pred):\n",
    "    \n",
    "    pred_probs = pd.Series(prediction_probabilities[:,1], index=X.index)\n",
    "    predictions = pd.Series(y_pred, index=y.index)\n",
    "    test_games = tourney_comp_ratings[tourney_comp_ratings.index.isin(X.index)].copy()\n",
    "    test_games[numeric_feature_to_scale] = scaler.inverse_transform(test_games[numeric_feature_to_scale])\n",
    "    test_games['predicted_result'] = predictions\n",
    "    test_games['pred_win_prob'] = pred_probs\n",
    "    missed_predictions = test_games[test_games['game_result'] != \n",
    "                                test_games['predicted_result']].sort_values(by='pred_win_prob', ascending=False)\n",
    "   \n",
    "    missed_predictions.apply(lambda x: feature_dictionary.print_game_info(test_games,x['season_t'], x['round'], x['team_t'] ), axis=1)\n",
    "    supporting_features = missed_predictions.apply(lambda row: utils.get_supporting_features(row,\n",
    "                                                                                         feature_dictionary, \n",
    "                                                                                         feature_list),axis=1)\n",
    "\n",
    "    supporting_model_features = missed_predictions.apply(lambda row: utils.get_supporting_features(row, \n",
    "                                                                                               feature_dictionary,\n",
    "                                                                                               model_features),axis=1)\n",
    "    \n",
    "    missed_predictions = missed_predictions.merge(supporting_features.to_frame(name='supporting_features'),how='left',\n",
    "                                              left_index=True, right_index=True)\n",
    "\n",
    "    missed_predictions = missed_predictions.merge(supporting_model_features.to_frame(name='supporting_model_features'),how='left', \n",
    "                                              left_index=True, right_index=True)\n",
    "\n",
    "    missed_predictions['features'] = 100 * missed_predictions['supporting_features'].apply(lambda x: len(x)) / len(feature_list)\n",
    "\n",
    "    missed_predictions['model_features'] = 100 * missed_predictions['supporting_model_features'].apply(lambda x: len(x)) / \\\n",
    "        len(model_features)\n",
    "\n",
    "    missed_predictions['game_index'] = missed_predictions.index\n",
    "    \n",
    "    return missed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_features_logistic_regression(classifier, X, y ):\n",
    "    iteration = 0\n",
    "    print(\"Iteration= \", iteration)\n",
    "    iteration += 1\n",
    "    model_stats = {}\n",
    "    drop_list = []\n",
    "    # get baseline by identifying sorted important features using all of the provided features\n",
    "    model_stats = utils.save_model_stats(classifier,X,y,model_stats)\n",
    "    important_features = utils.display_important_features(classifier.coef_[0], X,0)\n",
    "    #important_features = display_important_features_regression(classifier, X,0)\n",
    "    # least important feature\n",
    "    least_important_label = important_features[-1][0]\n",
    "    print(\"least_important label= \", least_important_label)\n",
    "    \n",
    "    drop_list.append(least_important_label)\n",
    "    del important_features[-1]\n",
    "    \n",
    "    # drop list contains all of the feature labels except for the feature label identified as being most important\n",
    "    list_count = len(important_features)\n",
    "    while list_count > 0:\n",
    "        print(\"Iteration= \", iteration)\n",
    "        iteration += 1\n",
    "        model_stats = utils.save_model_stats(classifier,X.drop(columns=drop_list),y,model_stats)\n",
    "        least_important_label = important_features[-1][0]\n",
    "        print(\"least_important label= \", least_important_label)\n",
    "        drop_list.append(least_important_label)\n",
    "        del important_features[-1]\n",
    "        list_count-=1\n",
    "    return model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_team_file = '../Data/sr_summaries_kaggle_id_no_opp_2018.csv'\n",
    "team_meta_data_file = '../Data/D1_teams.csv'\n",
    "tournament_data_file = '../Data/tournament_results_2018.csv'\n",
    "rankings_data_file = '../Data/massey_seasons_with_id.csv'\n",
    "\n",
    "feature_dictionary = utils.Feature_Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not use 2017-2018  for training, we'll hold that data back for testing a season in isolation\n",
    "\n",
    "# These dates correspond to the year in which the tournament was played.\n",
    "# For a start tournament date of 2003, the corresponding season is 2002-2003\n",
    "start_tournament = 2003\n",
    "stop_tournament = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in regular season team statistics from SRCBB https://www.sports-reference.com/cbb/\n",
    "\n",
    "#### Read table of team names and associated team meta data from the Kaggle data set.\n",
    "https://console.cloud.google.com/bigquery?project=bigqueryncaa&p=bigquery-public-data&d=ncaa_basketball&page=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The season field in the summary data corresponds to the starting year of the season (e.g. 2000 for 2000-2001 season)\n",
    "summary_data = file_utils.read_summary_team_data(summary_team_file)\n",
    "teams = file_utils.read_team_meta_data(team_meta_data_file)\n",
    "summary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary table above contains seasonal summary statistics for division one teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the starting season (not tournament year) for the summary data\n",
    "summary_data['season'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the NCAA Men's Tournament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The season in the tournamnet data corresponds to the year the tournament occurs (e.g. 2003 for season 2002-2003)\n",
    "tourney_data = file_utils.read_tournament_results(tournament_data_file,start_tournament)\n",
    "tourney_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = utils.compute_game_data(tourney_data, teams)\n",
    "\n",
    "# The season year in the computer rankings file correspond to the tournament year (e.g. 2003 for 2002-2003 season)\n",
    "computer_rankings = pd.read_csv(Path(rankings_data_file))\n",
    "computer_rankings = computer_rankings[computer_rankings['season'] >= start_tournament]\n",
    "\n",
    "# Recoding the tourney data to generate team and opp_team fields to replace win and lose fields\n",
    "# Also add a start season field to the tourney data for merging with summary data\n",
    "tourney_data = utils.recode_tourney_data(tourney_data)\n",
    "\n",
    "# Merge the tourney data with the summary data. Handle the discrepancy in the season encodings. \n",
    "tourney_data = file_utils.merge_tourney_summary_data(tourney_data, summary_data)\n",
    "\n",
    "tourney_data = file_utils.join_tourney_team_data(tourney_data, teams)\n",
    "\n",
    "# Add computer ranking data to team data\n",
    "tourney_comp_ratings = file_utils.merge_tourney_ranking_data(tourney_data, computer_rankings)\n",
    "tourney_comp_ratings = utils.implement_top_conference_feature(game_data, tourney_comp_ratings)\n",
    "tourney_comp_ratings = utils.implement_seed_threshold_feature(tourney_comp_ratings)\n",
    "tourney_comp_ratings = utils.compute_delta_features(tourney_comp_ratings)\n",
    "\n",
    "tourney_comp_ratings.dropna(inplace=True)\n",
    "tourney_comp_ratings[tourney_comp_ratings.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_comp_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_comp_ratings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['delta_margin_victory_avg', 'delta_fg_pct', 'delta_off_rebs_avg',\n",
    "                            'delta_def_rebs_avg', 'delta_ft_pct',\n",
    "                            'delta_to_net_avg', 'delta_win_pct', 'delta_off_rating',\n",
    "                            'delta_ft_att_avg',\n",
    "                            'delta_seed', 'delta_srs', 'delta_sos',\n",
    "                            'delta_sag', 'delta_wlk', 'delta_wol',\n",
    "                            'delta_rth', 'delta_col', 'delta_pom',\n",
    "                            'delta_dol', 'delta_mor']\n",
    "\n",
    "# Convert types to float to eliminate needless scaler warning \n",
    "for item in numeric_features:\n",
    "    tourney_comp_ratings[item] = tourney_comp_ratings[item].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = numeric_features  + ['season_t', 'top_conf_t', 'top_conf_o']\n",
    "feature_data = tourney_comp_ratings[feature_columns].copy()\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= feature_data[feature_data['season_t']<= stop_tournament]\n",
    "X['season_t'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tourney_comp_ratings[tourney_comp_ratings['season_t']<= stop_tournament]['game_result']\n",
    "X= X.drop(columns=['season_t'])\n",
    "\n",
    "feature_list = list(X)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 5)\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the input data.   \n",
    "#### Mention Data Snooping.\n",
    "Note the scaler is fit only with the training data to establish the mean and standard deviation used to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "scaler.fit(X_train[numeric_features])\n",
    "X_train[numeric_features] = scaler.transform(X_train[numeric_features])\n",
    "X_test[numeric_features] = scaler.transform(X_test[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "result = logreg.fit(X_train,y_train)\n",
    "\n",
    "print(\"Coeffs \",logreg.coef_)\n",
    "print(\"Intercept \", logreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_important_features(logreg.coef_[0], X_train,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities = logreg.predict_proba(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"Log loss= \",log_loss(y_test, prediction_probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()\n",
    "scaler.fit(X_copy[numeric_features])\n",
    "X_copy[numeric_features] = scaler.transform(X_copy[numeric_features])\n",
    "train_sizes, train_scores, test_scores = learning_curve(logreg, \n",
    "                                                        X_copy, \n",
    "                                                        y,\n",
    "                                                        # Number of folds in cross-validation\n",
    "                                                        cv=10,\n",
    "                                                        # Evaluation metric\n",
    "                                                        scoring='accuracy',\n",
    "                                                        # Use all computer cores\n",
    "                                                        n_jobs=-1, \n",
    "                                                        # 50 different sizes of the training set\n",
    "                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n",
    "\n",
    "# Create means and standard deviations of training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Draw lines\n",
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "# Draw bands\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(logreg, X_copy,y, cv=10, scoring='accuracy')\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Feature Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = eliminate_features_logistic_regression(logreg, X_copy,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy = 0\n",
    "max_cross_val = 0\n",
    "min_log_loss = 10000\n",
    "for key, value in model_stats.items():\n",
    "    accuracy = value['accuracy']\n",
    "    cross_val = value['cross_validation']\n",
    "    log_loss_val = value['log_loss']\n",
    "    print('Accuracy= {0:6.4f} Cross Val= {1:6.4f}  Log Loss= {2:6.4f}'.format(accuracy ,cross_val, log_loss_val ))\n",
    "    if accuracy > max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        accuracy_hash = key\n",
    "    if cross_val > max_cross_val:\n",
    "        max_cross_val = cross_val\n",
    "        cross_hash = key\n",
    "    if log_loss_val < min_log_loss:\n",
    "        min_log_loss = log_loss_val\n",
    "        log_hash = key\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print('Max Accuracy= {0:6.4f}'.format( model_stats[accuracy_hash]['accuracy']))\n",
    "print('Max Cross Validation= {0:6.4f}'.format( model_stats[cross_hash]['cross_validation']))\n",
    "print (\"Minimum Log Loss= {0:6.4f}\".format(  model_stats[log_hash]['log_loss']))\n",
    "print('Log Loss at Max Accuracy= {0:6.4f}'.format( model_stats[accuracy_hash]['log_loss'] ))\n",
    "print('Log Loss at Max Cross Validation= {0:6.4f} '.format( model_stats[cross_hash]['log_loss'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features =model_stats[cross_hash]['labels']\n",
    "\n",
    "print(model_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric model features are the intersection of numeric features with model features\n",
    "numeric_model_features = list(set(model_features).intersection(set(numeric_features)))\n",
    "\n",
    "numeric_model_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped features\n",
    "dropped_features = list(set(feature_list) - set(model_features))\n",
    "dropped_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model = X[model_features].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_model, y, test_size=0.2, random_state= 5)\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "scaler.fit(X_train[numeric_model_features])\n",
    "X_train[numeric_model_features] = scaler.transform(X_train[numeric_model_features])\n",
    "X_test[numeric_model_features] = scaler.transform(X_test[numeric_model_features])\n",
    "\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "# save model stats\n",
    "prediction_probabilities = logreg.predict_proba(X_test)\n",
    "\n",
    "cross_val_scores = cross_val_score(logreg, X,y, cv=10, scoring='accuracy')\n",
    "cross_validation_average = cross_val_scores.mean()\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"Log loss= \",log_loss(y_test, prediction_probabilities))\n",
    "\n",
    "utils.display_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missed_predictions = get_missed_predictions(tourney_comp_ratings, model_features, numeric_model_features, \n",
    "                                            prediction_probabilities, X_test, y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missed_predictions_df = missed_predictions[['game_index','features','model_features']]\n",
    "plot_missed_predictions_df = pd.melt(plot_missed_predictions_df, id_vars='game_index', var_name= 'Features Supporting Outcome')\n",
    "m_plot = sns.barplot(x='game_index', y='value', hue='Features Supporting Outcome', data= plot_missed_predictions_df) \n",
    "plt.title(\"Percentage Of Features Consistent With Game Outcomes\")\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Missed Prediction Game Index')\n",
    "m_plot.figure.set_size_inches(20,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The bar chart depicts the percentage of features that correctly corresponded to the game outcome but were out weighed by other features in predicting the game incorrectly. Games corresponding to bar heights exceeding 50% should be scrutinized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions[missed_predictions['game_index']==201]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model Against 2018 Tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = 2018\n",
    "\n",
    "# scale the feature data corresponding to the 2018 tournament\n",
    "feature_copy = feature_data[feature_data['season_t'] == test_year].copy()\n",
    "\n",
    "feature_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input data\n",
    "feature_copy[numeric_model_features] = scaler.transform(feature_copy[numeric_model_features])\n",
    "feature_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_season = feature_copy[model_features]\n",
    "\n",
    "y_season = tourney_comp_ratings[tourney_comp_ratings['season_t']== test_year]['game_result']\n",
    "X_season.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_season = logreg.predict(X_season)\n",
    "utils.display_confusion_matrix(y_season,y_pred_season)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities = logreg.predict_proba(X_season)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_season, y_pred_season))\n",
    "print(\"Precision:\", metrics.precision_score(y_season,y_pred_season))\n",
    "print(\"Recall:\",metrics.recall_score(y_season, y_pred_season))\n",
    "print(\"Log loss= \",log_loss(y_season, prediction_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions = get_missed_predictions(tourney_comp_ratings, model_features, numeric_model_features, \n",
    "                                            prediction_probabilities,X_season,y_season,y_pred_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missed_predictions_df = missed_predictions[['game_index','features','model_features']]\n",
    "plot_missed_predictions_df = pd.melt(plot_missed_predictions_df, id_vars='game_index', var_name= 'Features Supporting Outcome')\n",
    "m_plot = sns.barplot(x='game_index', y='value', hue='Features Supporting Outcome', data= plot_missed_predictions_df) \n",
    "plt.title(\"Percentage Of Features Consistent With Game Outcomes\")\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Missed Prediction Game Index')\n",
    "m_plot.figure.set_size_inches(20,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_predictions[missed_predictions['game_index']==1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that the probability of the first team winning is returned in the 2nd column of the prediction_probabilities array\n",
    "\n",
    "log_loss_result = utils.compute_log_loss(y_season.values, prediction_probabilities[:,1] )\n",
    "log_loss_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_probabilities[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What predictions were counter to seeding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_games = tourney_comp_ratings[tourney_comp_ratings.index.isin(X_season.index)].copy()\n",
    "tourney_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_games['predicted'] = y_pred_season\n",
    "tourney_games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_1 = (tourney_games['seed_t'] > tourney_games['seed_o']) & (tourney_games['predicted'] == 1)\n",
    "cond_2 = (tourney_games['seed_t'] < tourney_games['seed_o']) & (tourney_games['predicted'] == -1)\n",
    "\n",
    "predictions_counter_seed = tourney_games[cond_1 | cond_2]\n",
    "predictions_counter_seed[['round','seed_t','team_t','seed_o','team_o','game_result','predicted','win_pts','lose_pts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct counter seed predictions\n",
    "correct_counter_predictions = predictions_counter_seed[predictions_counter_seed['game_result']== predictions_counter_seed['predicted']]\n",
    "correct_counter_predictions[['round','seed_t','team_t','seed_o','team_o','game_result','predicted','win_pts','lose_pts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect counter seed predictions\n",
    "wrong_counter_predictions = predictions_counter_seed[predictions_counter_seed['game_result'] != predictions_counter_seed['predicted']]\n",
    "wrong_counter_predictions[['round','seed_t','team_t','seed_o','team_o','game_result','predicted','win_pts','lose_pts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of counter seed predictions= \", len(predictions_counter_seed.index))\n",
    "print(\"Number of correct counter seed predictions= \", len(correct_counter_predictions.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
