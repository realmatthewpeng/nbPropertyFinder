{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Profiler Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import random\n",
    "import datetime\n",
    "import math\n",
    "import json\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seaborn grid and figuresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set(rc = {\"figure.figsize\":(16,12)})\n",
    "seaborn.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab current timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get timestamp\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d\") + \"_\"\n",
    "\n",
    "print(\"Timestamp set to: {}\".format(timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "- Fill in the parameters within Settings subsections and run all cells to generate report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data settings\n",
    "- First import your data into a pandas dataframe (mandatory)\n",
    "- You can use `pandas`' `read_csv()`, `read_excel()`, `read_json()` methods to read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/users/danielcorcoran/desktop/\\\n",
    "github_repos/python_nb_visualization/\\\n",
    "seaborn_official_datasets/iris.csv\"\n",
    "\n",
    "data = pandas.read_csv(data_path)\n",
    "\n",
    "print(\"Dataframe has {} features and {} records\".format(data.shape[1], data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataframe feature names: {}\".format(list(data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export settings\n",
    "- Set your `exportpath`. The profiler will export the .csv/.json report as well as visualization .png/.svgs to this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportpath = \"/users/danielcorcoran/desktop/github_repos/python_nb_data_profiling/exports/\"\n",
    "\n",
    "print(\"Export path set to '{}'\".format(exportpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report Settings \n",
    "- Set report width and spacing chars (optional)\n",
    "- `report_width` is the width in chars of the report printed out in step `1.7 Data Profile Results`\n",
    "- `header_spacing_char` is the char used to pad the report column headers eg. \"##### age (1/12) #####\" for \"#\"\n",
    "- `attribute_spacing_char` is the char used to pad the attribute value pairing eg. \"mean:___34\" for \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_width = 80\n",
    "just_width = math.floor(report_width / 2)\n",
    "attribute_spacing_char = \" \"\n",
    "header_spacing_char = \"~\"\n",
    "\n",
    "print(\"Report width set to {}\".format(report_width))\n",
    "print(\"Attribute spacing char set to '{}'\".format(attribute_spacing_char))\n",
    "print(\"Header spacing char set to '{}'\".format(header_spacing_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Settings\n",
    "- `visualize_dataset` if this is set to True, the dataset will be visualized using `seaborn`'s `.pairplot()` and `heatmap()`\n",
    "- `deal_with_nulls` set this to 'mean' to imputate missing values, or 'remove' to drop them entirely.\n",
    "- `palette` seaborn palette. Visit https://seaborn.pydata.org/tutorial/color_palettes.html for more info.\n",
    "- `nunique_range` Categorical features with unique items between this range will be visualized in the `.pairplot()`\n",
    "- `features_to_exclude` A list of features to exclude from visualization data.\n",
    "- `identity_type` Set this to 'kde' for kernel density or 'hist' for histogram.\n",
    "- `non_identity_type` Set this to 'scatter' for scatterplot or 'reg' for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset = True\n",
    "features_to_exclude = ['x', 'y', 'z']\n",
    "deal_with_nulls = \"mean\"\n",
    "nunique_range = [2,10]\n",
    "palette = \"Dark2\"\n",
    "identity_type = 'hist'\n",
    "non_identity_type = 'scatter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call pandas describe method on dataframe\n",
    "describe = data.describe(include = \"all\")\n",
    "\n",
    "#transpose\n",
    "describe_transposed = describe.T\n",
    "\n",
    "#reset_index, moving the column names into a new series\n",
    "describe_final = describe_transposed.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datatypes of each feature\n",
    "dtypes = data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dataframe and rename a column\n",
    "dtypes_dataframe = pandas.DataFrame(dtypes, columns = [\"data_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index, moving the column names into a new series\n",
    "dtypes_final = dtypes_dataframe.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview\n",
    "dtypes_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Null composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use notnull() and isnull() methods combined with sum() to\n",
    "#get null composition of dataset\n",
    "filled_count_series = data.notnull().sum().astype(int)\n",
    "null_count_series = data.isnull().sum().astype(int)\n",
    "\n",
    "#get amount of rows in dataset\n",
    "totalcount = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create percentage series based on series above\n",
    "percent_null =  null_count_series / totalcount\n",
    "percent_filled = filled_count_series / totalcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_null = percent_null.reset_index().rename(columns = {0:\"null_percent\"})\n",
    "percent_filled = percent_filled.reset_index().rename(columns = {0:\"non_null_percent\"})\n",
    "filled_count_series = filled_count_series.reset_index().rename(columns = {0:\"non_null_counts\"})\n",
    "null_count_series = null_count_series.reset_index().rename(columns = {0:\"null_counts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_info_dataframe = reduce(lambda left,right: pandas.merge(left,right,on='index'), [percent_null,\n",
    "                                                                                 percent_filled,\n",
    "                                                                                 filled_count_series,\n",
    "                                                                                 null_count_series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_info_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Merge all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = reduce(\n",
    "    lambda left, right: pandas.merge(left, right, on='index', how=\"left\"),\n",
    "    [null_info_dataframe, dtypes_final, describe_final])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup: Drop count column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"row_count\"] = totalcount\n",
    "\n",
    "merged.drop([\"count\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup: Calculate unique values for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in gaps in unique column with nunique method\n",
    "\n",
    "for index in range(merged.shape[0]):\n",
    "    \n",
    "    unique_value = merged.loc[index, \"unique\"]\n",
    "    \n",
    "    if numpy.isnan(unique_value):\n",
    "        \n",
    "        feature_name = merged.loc[index, \"index\"]\n",
    "        \n",
    "        \n",
    "        number_of_uniques = data[feature_name].nunique()\n",
    "        merged.loc[index, \"unique\"] = number_of_uniques\n",
    "        print(\"Feature {} unique values being calculated and adjusted to {}\".format(feature_name, number_of_uniques))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup: Change types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing type of datatype column to string\n",
    "merged[\"data_type\"] = merged[\"data_type\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup: Round values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_datatypes = list(merged[\"data_type\"].unique())\n",
    "\n",
    "print(\"Unique datatypes in this dataset are: {}\".format(unique_datatypes))\n",
    "\n",
    "if \"float64\" in unique_datatypes or \"int64\" in unique_datatypes:\n",
    "    \n",
    "    round_values = True\n",
    "    rounded_places = 4\n",
    "    \n",
    "    for column in [\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\"]:\n",
    "        \n",
    "        merged[column] = merged[column].astype(float).round(rounded_places)\n",
    "else:\n",
    "    round_values = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Profile Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Store list of features relevant to dictionary\n",
    "merged_attributes = list(merged.columns)\n",
    "merged_attributes.remove(\"index\")\n",
    "\n",
    "# Iterate through merged records\n",
    "for index in range(merged.shape[0]):\n",
    "    \n",
    "    # Store column name\n",
    "    column = merged.loc[index, \"index\"]\n",
    "    \n",
    "    # Create empty list, to store sub dictionaries in\n",
    "    profile_dict[column] = []\n",
    "    \n",
    "    # For each attribute in merged_attributes list do this\n",
    "    for attr in merged_attributes:\n",
    "        \n",
    "        # Locate the value of the attribuet for a given record\n",
    "        value = merged.loc[index, attr]\n",
    "        \n",
    "        # Append dictionary to list within main dictionary\n",
    "        profile_dict[column].append({attr: value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results as .json and .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function to convert invalid json values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will try to convert a value to a float, followed by string,\n",
    "# if the float conversion fails\n",
    "\n",
    "def convert_invalid_values(value):\n",
    "    \n",
    "    try:\n",
    "        new_value = float(value)\n",
    "    except:\n",
    "        new_value = str(value)\n",
    "\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(exportpath + \"profile.json\", \"w\") as fileobj:\n",
    "    # for errors I want to attempt to convert the value to a float before str using\n",
    "    # the function defined above\n",
    "    json.dump(profile_dict, fileobj, default = convert_invalid_values)\n",
    "    \n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(exportpath + \"profile.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profile Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_keys = len(profile_dict.keys())\n",
    "column_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in profile_dict.keys():\n",
    "    \n",
    "    # Increment column number\n",
    "    column_number = column_number + 1\n",
    "    \n",
    "    # Calculate column pos\n",
    "    column_pos = \" ({}/{})\"\n",
    "    \n",
    "    print(\"\\n\", (\" \" + key + \" \").center(report_width, header_spacing_char))\n",
    "    \n",
    "    sub_dictionary = profile_dict[key]\n",
    "    for dictionary in sub_dictionary:\n",
    "        \n",
    "        keys = list(dictionary.keys())\n",
    "        attribute = keys[0]\n",
    "        value = dictionary[attribute]\n",
    "        \n",
    "        if \"percent\" in attribute:\n",
    "            formatted_value = \"{0:.2%}\".format(value)\n",
    "        else:\n",
    "            formatted_value = str(value)\n",
    "        \n",
    "        print(attribute.ljust(just_width, attribute_spacing_char), \n",
    "              formatted_value.rjust(just_width, attribute_spacing_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "(Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create subset of data without nulls for visualizing\n",
    "- Will react to `deal_with_nulls` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"this\"\n",
    "\n",
    "if d in [\"this is\", \"that\"]:\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(merged[(merged[\"data_type\"] == \"int64\") | \n",
    "            (merged[\"data_type\"] == \"float64\")][\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_dataset == True:\n",
    "    \n",
    "    if deal_with_nulls == \"remove\":\n",
    "        \n",
    "        # Print message\n",
    "        print(\"Nulls will be removed from visualization dataset.\")\n",
    "        \n",
    "        # Remove each record containing any nulls\n",
    "        data_no_nulls = data.dropna(how = \"any\", axis = 0)\n",
    "\n",
    "        print(\"Shape of original dataset {}\"\\\n",
    "              .format(data.shape))\n",
    "        print(\"Shape of visualization dataset {}. \\n{} Records were removed (containing nulls).\"\\\n",
    "              .format(data_no_nulls.shape, data.shape[0] - data_no_nulls.shape[0]))\n",
    "    else:\n",
    "        # Print message\n",
    "        \n",
    "        print(\"Nulls will be replaced by the mean of each numeric feature.\")\n",
    "        \n",
    "        # Find each column of numeric quality and store in list\n",
    "        columns_to_fill_mean = list(\n",
    "                merged[(merged[\"data_type\"] == \"int64\") | \n",
    "                (merged[\"data_type\"] == \"float64\")][\"index\"])\n",
    "        \n",
    "        data_no_nulls = data\n",
    "        \n",
    "        # For each column in list fill the dataset with the mean of that column\n",
    "        for column in columns_to_fill_mean:\n",
    "            data_no_nulls[column] = data_no_nulls[column].fillna(data_no_nulls[column].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns from visualization dataset\n",
    "- These features are listed in `features_to_exclude` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_nulls_column_list = list(data_no_nulls.columns)\n",
    "\n",
    "if visualize_dataset == True:\n",
    "    for feature in features_to_exclude:\n",
    "        if feature in data_no_nulls_column_list:\n",
    "            data_no_nulls.drop([feature], axis = 1, inplace = True)\n",
    "            print(\"{} removed from visualization dataset\".format(feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate chosen_categorical_features (features with unique number of items between given range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_dataset == True:\n",
    "\n",
    "    # Create subset of merged containing data type = object and nunique between range above\n",
    "    chosen_categorical_features = list(merged[(merged[\"unique\"] >= nunique_range[0]) & \n",
    "                               (merged[\"unique\"] <= nunique_range[1]) & \n",
    "                               (merged[\"data_type\"] == \"object\")][\"index\"])\n",
    "\n",
    "    print(\"Categorical features: {}\".format(chosen_categorical_features))\n",
    "\n",
    "    # Remove unwanted features\n",
    "    for feature in features_to_exclude:\n",
    "        if feature in chosen_categorical_features:\n",
    "            chosen_categorical_features.remove(feature)\n",
    "\n",
    "    # Preview chosen_categorical_features\n",
    "    print(\"Categorical features being used in pairplot: {}\".format(chosen_categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate which columns can be used as categories to split float data in pairplots\n",
    "- Note: code hasn't accounted for datasets without float/int features yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert bool dtypes before visualizing, as this produces errors otherwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data_no_nulls.columns:\n",
    "    dtype = data_no_nulls[column].dtype\n",
    "    if dtype == \"bool\":\n",
    "        data_no_nulls[column] = data_no_nulls[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if visualize_dataset == True:\n",
    "    \n",
    "    # If dataset contains at least one categorical feature do this\n",
    "    if \"unique\" in merged.columns:\n",
    "\n",
    "        for column in chosen_categorical_features:\n",
    "\n",
    "            plt.figure()\n",
    "            myplot = seaborn.pairplot(data = data_no_nulls, \n",
    "                                      kind = non_identity_type,\n",
    "                                      diag_kind = identity_type,\n",
    "                                      hue = column,\n",
    "                                      palette = palette\n",
    "                                     )\n",
    "\n",
    "            myplot.fig.suptitle(\"Pairplot categorized by {}\".format(column), \n",
    "                         y = 1.03)\n",
    "\n",
    "            plt.savefig(exportpath + timestamp +  column + \"_pairplot.png\")\n",
    "\n",
    "    # If data set has no categorical features (only floats/ints) do this\n",
    "    else:\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        myplot = seaborn.pairplot(data = data_no_nulls, \n",
    "                                  kind = non_identity_type,\n",
    "                                  diag_kind = identity_type\n",
    "                                 )\n",
    "\n",
    "        myplot.fig.suptitle(\"Pairplot\", \n",
    "                     y = 1.03)\n",
    "\n",
    "        plt.savefig(exportpath + timestamp +  \"data_profile_pairplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.set(rc = {\"figure.figsize\":(13,10)})\n",
    "seaborn.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_dataset == True and round_values == True:\n",
    "    # Save path\n",
    "    heatmap_save_path = exportpath + timestamp + \"heatmap.png\"\n",
    "    \n",
    "    # Create correlation Matrix\n",
    "    correlation_dataframe = data_no_nulls.corr()\n",
    "    \n",
    "    # Create mask\n",
    "    mask = numpy.zeros_like(correlation_dataframe)\n",
    "    mask[numpy.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Create heatmap, show and export as .png\n",
    "    seaborn.heatmap(data = correlation_dataframe,\n",
    "               cmap = ['#b2182b',\n",
    "                    '#ef8a62',\n",
    "                    '#fddbc7',\n",
    "                    '#f7f7f7',\n",
    "                    '#d1e5f0',\n",
    "                    '#67a9cf',\n",
    "                    '#2166ac'],\n",
    "               center = 0,\n",
    "               square = True,\n",
    "               linewidth = 1,\n",
    "               mask = mask,\n",
    "               annot = True).get_figure().savefig(heatmap_save_path)\n",
    "    \n",
    "    print(\"Heatmap saved to '{}'\".format(heatmap_save_path))\n",
    "else:\n",
    "    print(\"No heatmap was produced. Dataset contains no numeric features or visualize_dataset variable was set to False.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "327px",
    "width": "257px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
